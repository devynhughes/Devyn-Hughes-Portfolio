---
title: "COVID-19"
author: "Devyn Hughes, Lisa GreenPope"
date: "March 22, 2024"
output:
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
  html_notebook:
    toc: yes
    toc_depth: '2'
    df_print: paged
subtitle: IST707 Final Project
---

# Introduction

The COVID-19 pandemic is something that has impacted the lives of billions of people across the globe. It began in December 2019, when a small cluster of individuals who had shopped at a certain market in Wuhan, China began to experience symptoms of an unknown respiratory illness. Within a few weeks, the virus causing the illness was identified as a novel Coronavirus, which was first named SARS CoV-2, and then eventually COVID-19. It soon began to spread worldwide, with the first confirmed case appearing in the United States on January 20, 2020. The US and other countries responded to the outbreak in a variety of ways, starting with isolation and quarantine, travel restrictions, the declaration of Public Health Emergencies, social distancing and masking measures, and eventually progressing to more severe limitations such as school closures and stay-at-home orders. By April of 2020 there had been over 18,000 deaths due to COVID-19 in the United States, and by June there were over 2 million deaths.

Throughout the global torment of COVID-19, data was always constantly being collected each day. We all know that data science and analytics are key tools to today's society. Thus, how can we use this process for this potent and deadly virus? Before moving into that, questions had to be asked about the virus. What are the discerning factors between different countries? How did other countries handle the virus? Was there a particular infection rate that differed from others? There are hundreds of questions with hundreds of answers. Joshua Lederberg said, "The single, biggest threat to man's continued dominance on the planet is the virus.". Therefore, it is important to be able to understand, acknowledge, learn, and improve in the coming years to make sure economies can handle another issue if one arose.

When COVID-19 hit, testing was an important feature of curbing the spread of the illness. The more time that passed, the more detrimental it became for society. The virus had its own defense mechanism against testing as handle such an infectious terrorist is dangerous in its own right. Over time, countries were able to find ways to assess for COVID-19 which went from private testing sites, to public testing sites, to home testing kits. This was a huge step for the war against this resilient bane of existence. From further gather, testing, and ingenuity, vaccinations were created to top the spread of the illness. Many were in development and soon became public in December of2020 to those who were present around the virus on a daily basis, the healthcare workers.

However, one idea that comes to mind is, how can we improve from our faults? How do identify our issues and make better protocols and procedures for the next crisis? COVID-19 has now been considered as something similar to a common cold or a mild flu. There are no more needs of segregating oneself from society for 2 weeks like before. The CDC has stated that someone can be back to work in just a couple of days after experiencing symptoms now. With the data collected, we are looking to derive a problem statement that evaluates more about COVID-19 regarding many different factors across the world.

## Problem Statement

The last time the world experienced a global pandemic was approximately 100 years before the current one, in 1918-1920. The Spanish Flu pandemic was responsible for somewhere between 50-100 million deaths worldwide. In the early 1900's, society did not have the advanced healthcare system that is seen today, nor was the field of epidemiology as advanced as it is today. Even with the advanced knowledge available in 2020, the world was still not prepared for a pandemic of this scale. It will be important for public health officials to learn as much as possible from COVID-19, in order to be prepared for any future pandemics.

In this study, the mortality (death rate), and incidence (case rate) of the disease will be the primary outcomes assessed. The available data will be evaluated to look for any trends that identify which factors lead to increased mortality and increased incidence of disease.

# About the Data

The COVID-19 pandemic generated massive amounts of data. One important dataset comes from the UK based Global Change Data Lab's "Our World in Data" (OWID) project. The OWID project tracked key indicators of COVID-19 at the global level, and made that information available to the public. In this analysis, the OWID COVID-19 dataset as of March 5, 2022 was utilized. This dataset includes 166,326 observations of 67 different variables. Each observation represents a single country's status on a particular day, ranging from Afghanistan on 2/24/2020 to Zimbabwe on 3/5/22. For each country's daily observation, the 67 attributes measured fall into several key categories including:

-   Country attributes
-   Case counts
-   Epidemiological factors
-   Deaths
-   Hospital data
-   Testing data
-   Vaccination data

## Data Preparation

In data science, data cleaning and preparation is often the most labor intensive part of any analysis. This study is no different. One important issue to overcome with this data set is identifying how much data is missing, and then deciding how to handle those NA values. A good rule of thumb is that if over 5% of the data is missing, it may not be reliable. In this dataset, there are many observations and many attributes with well over 5% of the data missing.

Some data is missing due to the timeframe of the pandemic itself. For example, there is limited new case data until late January 2020, and no vaccination data until December 2020. There are some data points, such as those related to ICU admissions that are lacking across the board.

Another important issue is normalizing the data. This data includes both raw counts and rates, some of which are smoothed. The raw counts may be misleading because a country with a large population will most likely have more cases and deaths than a country with a small population. It would be more meaningful to compare the rates of disease and death, than to compare the actual counts. For this reason, the variables showing raw counts were excluded from analysis. Smoothed variables were also excluded, except for those related to vaccinations because in this data set, vaccination rates are only presented as smoothed.

### Load libraries

The first step of any data analysis project is to load all of the packages needed for analysis. In this study, a variety of packages used for data pre-processing, machine learning and visualization will be loaded.

```{r LoadLib, message=FALSE, warning=FALSE}
start_time <- Sys.time() #adding this just to track how long this takes

library(tidyverse, quietly = TRUE) #for data preparation 
library(arules, quietly = TRUE) #for association rule mining
library(arulesViz, quietly = TRUE) #for association rule mining
library(lsa, quietly = TRUE) #for distance measures in clustering
library(cluster, quietly = TRUE) #for clustering
library(foreign, quietly = TRUE) #to read in arff files 
library(factoextra, quietly = TRUE) #helps with hierarchical clustering
library(psych, quietly = TRUE)
library(gridExtra, quietly = TRUE) #to put multiple graphs into one image
library(cowplot, quietly = TRUE) #to put multiple graphs into one image
library(maps, quietly = TRUE) #to load geospatial data for maps
library(caret, quietly = TRUE) #variable importance in decision trees
library(rpart, quietly = TRUE) #helps with decision trees 
library(rpart.plot, quietly = TRUE) #decision tree visuals
library(xts, quietly = TRUE) #for time series 
library(tseries, quietly = TRUE) #for time series
library(rattle, quietly = TRUE) #for decision tree visualization 
library(janitor, quietly = TRUE) #to remove constants in a sparse matrix 
library(e1071, quietly = TRUE) #for svm & naive bayes
library(naivebayes, quietly = TRUE)
library(randomForest, quietly = TRUE)
library(ggfortify, quietly = TRUE) #to create principal components analysis plots
library(viridis, quietly = TRUE) #for color scales
library(class, quietly = TRUE) #for knn
library(fastDummies, quietly = TRUE) #to create dummy variables
library(scales, quietly = TRUE) #to format numeric data 
```

### Load & Tidy Data

The dataset was downloaded from Our World in Data, and saved as a CSV file.

```{r LoadData, warning=FALSE, message=FALSE}

#Read in the data (if using Devyn's computer)
covid <- read_csv("C:/Users/Devs PC/OneDrive/Documents/IST 707/owid-covid-data.csv")

#Read in the data (if using Lisa's computer)
#covid <- read_csv("owid-covid-data.csv")

#Convert the date column to a date format (not character)
covid$date <- as.Date(covid$date, format = "%m/%d/%Y")

#convert all character columns to factors
covid <- covid %>%
  mutate_if(is.character, as.factor) 

#create as a separate dataframe
df <- covid
```

## Exploratory Data Analysis

The COVID-19 dataset used in the analysis has many interesting variables. Before building any machine learning models, the data will be reviewed to look for trends that might inform model development. In particular, variables that impact the daily mortality rate will be assessed.

```{r EDAPlots1, warning=FALSE}
#create several plots
plot_hosp_death <- ggplot(df, aes(hosp_patients_per_million, new_deaths_per_million)) + 
  geom_point() + 
  coord_cartesian(ylim = c(0, 70)) + 
  labs(x = "Hospital patients per million", y = "New deaths per million", title = "Daily Hospitalizations vs Deaths")

plot_string_death <- ggplot(df, aes(stringency_index, new_deaths_per_million)) + 
  geom_point() + 
  coord_cartesian(ylim = c(0, 200)) + 
  labs(x = "Stringency Index", y = "New deaths per million", title = "Stringency Index vs Deaths")

plot_vax_death <- ggplot(df, aes(people_vaccinated_per_hundred, new_deaths_per_million)) + 
  geom_point() + 
  coord_cartesian(ylim = c(0, 300)) + 
  labs(x = "People vaccinated per 100", y = "New deaths per million", title = "Vaccinations vs Deaths")

plot_repro_death <- ggplot(df, aes(reproduction_rate, new_deaths_per_million)) + 
  geom_point() + 
  coord_cartesian(ylim = c(0, 250)) + 
  labs(x = "Viral Reproduction Rate", y = "New deaths per million", title = "Reproduction Rate vs Deaths")

#Visualize all four plots
cowplot::plot_grid(plot_hosp_death, plot_string_death, plot_vax_death, plot_repro_death, ncol = 2, nrow = 2)
```

The four plots above begin to show some interesting trends that can be further explored. The daily mortality rate is the dependent variable in these examples. It does appear that the daily mortality rate increases as the hospitalization rate increases. This is logical, as the more people who become ill enough to require hospitalization, the more people are likely to succumb to the illness.

Deaths and the stringency index also appear to be somewhat positively correlated as well. Recall that the stringency index is a measure of how strict a country was in terms of prevention measures, such as closing schools and public transportation, requiring masking and social distancing and other rules. Using epidemiological knowledge, it may be that the stringency measure is actually the dependent variable, and the more deaths that a country is experiencing, the government for that country may be reacting and enacting more strict regulations.

It is also interesting to note that as vaccinations increased, deaths appear to decrease. This makes sense, as a population approaches herd immunity with a greater portion of it's population vaccinated, people will be likely to experience less severe illnesses and less death.

The viral reproduction rate refers to how many individuals become ill from one infected individual. This is a measure of how rapidly a virus is spreading in the population. A reproduction number of one means that the rate of infection is likely remaining steady, with each person only infecting one additional person. When the reproduction number is higher, such as 2 or 3, it means that each infected individual is spreading the illness to several people, and the infection could spread exponentially faster. It appears that the death rate is highest when the reproduction number is around 1, which is interesting. When the virus is spreading faster, it is not necessarily more deadly. This is supported by the science as well, because some strains of the virus that were circulating were able to spread more rapidly but did not cause as severe of illness.

Deaths are often a function of how many cases are occurring, so the stringency index and cases will also be reviewed.

```{r EDAPlots2}
#Plot stringency against new cases 
ggplot(df, aes(stringency_index, new_cases_per_million)) + 
  geom_point() + 
  coord_cartesian(ylim = c(0, 35000))+ 
  labs(x = "Stringency Index", y = "New cases per million", title = "Stringency Index vs New Cases")
```

It is interesting to note that the number of cases occurring as a function of the stringency index appears to be more of a bell curve in shape. As discussed above, the stringency index may actually be a reactive measure, where as deaths increase, a country's response becomes more strict. That trend is not necessarily seen with cases. The number of new cases per day seems to taper out at both the low and high end of the stringency index.

### Outcome Selection: Mortality

Now that some basic exploratory data analysis has been conducted, mortality does appear to be a good outcome measure to use. There are some interesting trends that can be explored. Before that can be certain, it is important to assess the data quality, such as looking at the overall distribution, and how much data is missing.

```{r Mortality}
#Create histogram to view distribution of mortality rate
#Limit the x-axis range for easier viewing 
hist(df$new_deaths_per_million, breaks = 2000, xlim = c(0,10), main = "New Daily Deaths per Million Citizens", xlab = "Daily Deaths per Million Citizens")

#Calculate range of daily mortality rate 
range(na.omit(df$new_deaths_per_million))

#Calculate mean of daily mortality rate 
mean(na.omit(df$new_deaths_per_million))

#identify the 10% quantiles of daily mortality rate
quantile(df$new_deaths_per_million, probs = seq(0, 1, 1/10), na.rm = TRUE)

#what percent of daily death rates are NA?
1-(length(na.omit(df$new_deaths_per_million))/length(df$new_deaths_per_million))

#how many of the daily death rate observations are outliers?
length(boxplot(df$new_deaths_per_million)$out)

#view boxplot to visualize outliers
#What percentage are considered outliers?
length(boxplot(df$new_deaths_per_million)$out)/length(na.omit(df$new_deaths_per_million))

#Visualize population vs mortality rate to see if the outliers are primarily small countries (small numbers issue)
plot_pop_death <- ggplot(df, aes(population, new_deaths_per_million)) + 
  geom_point() + 
  coord_cartesian(ylim = c(0, 250)) + 
  labs(x = "Population", y = "New deaths per million", title = "Population vs Deaths") + 
  scale_x_continuous(labels = comma)
plot_pop_death
```

------------------------------------------------------------------------

Although the range for daily mortality ranges from 0 to 453 deaths per million citizens, the distribution is very skewed. The mean is 1.68 per million, per day, but it is not until after the 70th percentile that the death rate even reaches 1 per million per day. In addition to that, 13% of the values are NAs, and another 13% are outliers (beyond 1.5x the IQR). From reviewing the dataframe, the outliers appear to be coming from countries with smaller populations, where even one deaths might make the rate (per million) appear artificially high. This is one factor that will need to be considered during the analysis.

### Outcome Selection: Cases

The distribution of cases will also be reviewed, to assess if that could be used as an outcome variable as well.

```{r Cases}
#Create histogram to view distribution of case rate
hist(df$new_cases_per_million, main = "New Daily Cases per Million Citizens", xlab = "Daily Cases per Million Citizens")

#Calculate range for daily case rate 
range(na.omit(df$new_cases_per_million))

#Calculate mean for daily case rate
mean(na.omit(df$new_cases_per_million))

#identify the 10% quantiles of daily case rate
quantile(df$new_cases_per_million, probs = seq(0, 1, 1/10), na.rm = TRUE)

#Count non-blank daily case rate observations 
length(na.omit(df$new_cases_per_million))

#count total observations (including blanks) for daily case rate 
length(df$new_cases_per_million)

#what percent of daily case rates are NA?
1-(length(na.omit(df$new_cases_per_million))/length(df$new_cases_per_million))
```

The distribution for cases is similar to deaths, there is a strong skew with a large portion of the days with numbers at either 0 or very close to 0 even though the mean is 166 cases per million, per day. This is due to some very high outliers, with the range of values being from 0 to 51,427. One positive thing about the case rate data is that there are relatively fewer blank values, with only 2% NAs.

### Worldwide Geography

Next, the data will be viewed through the lens of geography. One particular date was chosen to create a cross section and review the daily number of cases per million citizens on that day, broken down by continent for easier visualization.

```{r Geo}
#Create a table with just one month 
df_Aug2121 <- df %>%
  filter(date == "2021-08-21") %>%
  filter(!is.na(continent))

#count the number of countries reporting data  
length(unique(df_Aug2121$location))

#Create a plot of cases in South America
ggplot(df_Aug2121[df_Aug2121$continent=="South America",], aes(location, new_cases_per_million)) +
  geom_col() + 
  labs(title = "South America: New cases per million on 8/21/21") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  ylim(0,1250)

#Create a plot of cases in Asia 
ggplot(df_Aug2121[df_Aug2121$continent=="Asia",], aes(location, new_cases_per_million)) +
  geom_col() + 
  labs(title = "Asia: New cases per million on 8/21/21") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+ 
  ylim(0,1250)

#Create a plot of cases in Europe 
ggplot(df_Aug2121[df_Aug2121$continent=="Europe",], aes(location, new_cases_per_million)) +
  geom_col() + 
  labs(title = "Europe: New cases per million on 8/21/21") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  ylim(0,1250)

#Create a plot of cases in Africa 
ggplot(df_Aug2121[df_Aug2121$continent=="Africa",], aes(location, new_cases_per_million)) +
  geom_col() + 
  labs(title = "Africa: New cases per million on 8/21/21") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  ylim(0,1250)

#Create a plot of cases in North America 
ggplot(df_Aug2121[df_Aug2121$continent=="North America",], aes(location, new_cases_per_million)) +
  geom_col() + 
  labs(title = "North America: New cases per million on 8/21/21") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  ylim(0,1250)

#Create a plot of cases in Oceania 
ggplot(df_Aug2121[df_Aug2121$continent=="Oceania",], aes(location, new_cases_per_million)) +
  geom_col() + 
  labs(title = "Oceania: New cases per million on 8/21/21") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  ylim(0,1250)

```

The charts above were all scaled similarly so that they could be compared. It's clear that there is a wide variety of case rates on that day, with many countries showing no cases, and some showing over 1,000 cases per million citizens.

### United States

Next, the data will be further sliced and diced to look at one particular country over time. In this case, we will review data from the United States .

```{r USA, warning=FALSE}
df_USA <- df %>%
  filter(location == "United States")

plotUSCases <- ggplot(df_USA, aes(x = date, y = new_cases_per_million)) +
  geom_line() + 
  labs(title = "US: New cases per million", y = "New Cases") 

plotUSdeaths <- ggplot(df_USA, aes(x = date, y = new_deaths_per_million)) +
  geom_line() + 
  labs(title = "US: New deaths per million", y = "Deaths")

plotUSvax <- ggplot(df_USA, aes(x = date, y = new_vaccinations_smoothed_per_million)) +
  geom_line() + 
  labs(title = "US: New vaccinations smoothed per million",
       y = "Vaccinations Smoothed")

cowplot::plot_grid(plotUSCases, plotUSdeaths, plotUSvax, ncol = 1, nrow = 3)
```

The time series data above shows several interesting trends. First, it can be seen that the number of cases had some seasonal spikes around January 2021 and January 2022. Additionally, there are smaller, more frequent fluctuations in the data as well. This is likely due to reporting structures, where more data was reported on weekdays than on weekends, so case and death rates seemed artificially low on weekends. A final interesting point is that the introduction of vaccinations does not occur until the end of 2020, so there is no data available prior to that time point.

# Models

## Unsupervised Learning: Clustering

Creating this table to get the demographic data for each country, so we can cluster them. We're collapsing it down using distinct() so that there is no time series data here, just one line per country. We'll drop any country that has an NA value in one of these areas, because that messes up our k-means clustering later.

```{r Country}
#Create dataframe by country 
df_covid_country <- df %>%
  distinct(location, .keep_all = TRUE) %>%
  select(continent,
          location,
          hospital_beds_per_thousand,
          life_expectancy,
          human_development_index,
          gdp_per_capita,
          extreme_poverty,
          median_age)%>%
  drop_na()


#Make an unlabeled version 
df_covid_country_UL <- df_covid_country[,3:8]

#Make the rownames the labels, it will help later with the visualization 
rownames(df_covid_country_UL)<- df_covid_country$location

#Check for NAs 
sum(is.na(df_covid_country_UL))

#elbow method to decide how many clusters to use
fviz_nbclust(df_covid_country_UL, kmeans, method = "wss")

#Create a k-means model 
set.seed(9) 
df_covid_country_KM <- kmeans(df_covid_country_UL, 3)

#view details about the model
df_covid_country_KM[2:9]

#add the new clusters back to the original data
df_covid_country <- data.frame(df_covid_country, cluster = df_covid_country_KM$cluster)
```

For our clustering, an optimizing method was used to determine the number of clusters needed for this evaluation. From 1 to 10, we saw that a total of 3 clusters seemed to yield the most decisive information for all of our data. Once again, any countries that had a yield of "NA" were removed.

### Map of countries in each demographic cluster

```{r Maps}
#load in the geospatial data
world_map <- map_data("world")

#add a column to df_covid_country where country is labeled as "region" so it will merge with the map data
df_covid_country$region <- df_covid_country$location

#change region to USA instead of United States so it can merge
df_covid_country$region <- str_replace(df_covid_country$region, "United States", "USA")
df_covid_country$region <- str_replace(df_covid_country$region, "United Kingdom", "UK")

#merge it with the df_covid_country data
merged_world <- full_join(world_map, df_covid_country, by = "region")

#make cluster a factor, when we merged them cluster was formatted as numeric which made the map have a color scale instead of discrete colors
merged_world$cluster <- as.factor(merged_world$cluster)

#create the map
ggplot() + 
  geom_polygon(data = merged_world, aes(x=long, y=lat, group=group, fill = cluster), color = "black") + 
  labs(title = "Demographic Clusters")


```

After creating the 3 cluster basis, we then applied a plot to better visualize which countries or areas that represent each of the clusters outlined on the world map. From a simple visualization, Cluster 1 includes places like Russia, Brazil, Central America, etc. For cluster 2, it includes countries within Africa, South America, and Eastern Asia. For Cluster 3, we have, most noticeably, the United States and Australia. The final cluster is the NA cluster. Essentially, this is a cluster that contained the removed values due to NA data being listed. Next, we will evaluate what each of these clusters have in common over their groupings.

### Box plots for country demographic clusters

There are 3 clusters, with 6 different metrics that contributed

```{r BoxPlots}
#first, make cluster a factor 
df_covid_country$cluster <- as.factor(df_covid_country$cluster)

#then create the plot 
plot_hb <- ggplot(df_covid_country, aes(cluster, hospital_beds_per_thousand)) +
  geom_boxplot() + 
  labs(title = "Hospital beds per thousand", y ="Beds")

plot_le <- ggplot(df_covid_country, aes(cluster, life_expectancy)) +
  geom_boxplot() + 
  labs(title = "Life expectancy", y = "Years")

plot_hdi <- ggplot(df_covid_country, aes(cluster, human_development_index)) +
  geom_boxplot() + 
  labs(title = "Human Development Index", y = "Index")

plot_gdp <- ggplot(df_covid_country, aes(cluster, gdp_per_capita)) +
  geom_boxplot() + 
  labs(title = "GDP per Capita", y = "GDP")

plot_ep <- ggplot(df_covid_country, aes(cluster, extreme_poverty)) +
  geom_boxplot() + 
  labs(title = "Extreme Poverty", y = "% of Pop")

plot_ma <- ggplot(df_covid_country, aes(cluster, median_age)) +
  geom_boxplot() + 
  labs(title = "Median Age", y = "Years")

#Visualize all the plots created 
cowplot::plot_grid(plot_ep, plot_gdp, plot_hb, plot_hdi, plot_le, plot_ma, ncol = 2, nrow = 3)
```

In order to determine the parameters of each of our clusters, we had to determine the different levels of variables between all of our attributes. The attributes assessed were GDP, HDI, Life Expectancy, Age, Poverty, and Life Expectancy.

Cluster 1:

-   Mid Range GDP
-   Mid HDI
-   Mid Life Expectancy

Cluster 2:

-   Low GDP
-   Low HDI
-   Low Life Expectancy

Cluster 3:

-   Highest GDP
-   Highest HDI
-   Highest Life Expectancy \### Creating the countries from different clusters

```{R DH1}
#Cluster 1 - Red

df_Russia <- df %>%
  filter(location == "Russia")

#Cluster 2 - Green
df_China = df %>%
  filter(location == "China")

#Cluster 3 - USA
```

For the next part of our analysis, there were some distinct countries within each of the clusters as we stated before. We determined that there are different economical levels and other variables between all three of the clusters. However, how does these three clusters compare via cases, deaths, and vaccines? Our next tool will utilize a time lapse graph that shows how the amounts changed for all three scenarios.

### Creating Time Lapse Graphs based on each cluster

```{r DH2}
#Time Lapse of Cases per Cluster
tlRus <- ggplot(df_Russia, aes(x = date, y = new_cases_per_million)) +
  geom_line() + 
  labs(title = "RUS: New cases per million", y = "New Cases") 

tlChi <- ggplot(df_China, aes(x = date, y = new_cases_per_million)) +
  geom_line() + 
  labs(title = "CHI: New cases per million", y = "New Cases")

tlUSA <- ggplot(df_USA, aes(x = date, y = new_cases_per_million)) +
  geom_line() + 
  labs(title = "New cases per million",
       y = "New Cases")

cowplot::plot_grid(tlRus, tlChi, tlUSA, ncol = 1, nrow = 3)

```

The first comparison was solely generated from the cases of the United States, Russia, and China. The date starts from January 2020 and goes all the way to beginning of 2023. Based on the visual, we can see that China had most of their newer cases present in between January 2020 and May 2020. As for Russia and US, the cases for COVID mostly peaked and spiked in 2021. A question that arises is how China was able to contain it so well? Was this due to the virus starting in China? The drastic difference in foreign policies and utilizations via government are shown truly within these time lapses.

```{r DH3}
#Time Lapse Deaths per Cluster

tlRus2 <- ggplot(df_Russia, aes(x = date, y = new_deaths_per_million)) +
  geom_line() + 
  labs(title = "RUS: New Deaths per million", y = "New Deaths") 

tlChi2 <- ggplot(df_China, aes(x = date, y = new_deaths_per_million)) +
  geom_line() + 
  labs(title = "CHI: New Deaths per million", y = "New Deaths")

tlUSA2 <- ggplot(df_USA, aes(x = date, y = new_deaths_per_million)) +
  geom_line() + 
  labs(title = "New Deaths per million",
       y = "New Deaths")

cowplot::plot_grid(tlRus2, tlChi2, tlUSA2, ncol = 1, nrow = 3)
```

The next timelapse covers the Deaths per million between all three countries. For Russia, it was gradually escalating as time increased. China had a sudden spike of deaths in 2020. Lastly, USA had a peaks and troughs during the whole timeline, varying greatly unlike the others.

```{R DH4}
#Time Lapse Vaccinations per Cluster

tlRus3 <- ggplot(df_Russia, aes(x = date, y = new_vaccinations_smoothed_per_million)) +
  geom_line() + 
  labs(title = "RUS: New Vaccinations per million", y = "New Vaccinations") 

tlChi3 <- ggplot(df_China, aes(x = date, y = new_vaccinations_smoothed_per_million)) +
  geom_line() + 
  labs(title = "CHI: New Vaccionations per million", y = "New Vaccinations")

tlUSA3 <- ggplot(df_USA, aes(x = date, y = new_vaccinations_smoothed_per_million)) +
  geom_line() + 
  labs(title = "New Vaccinations per million",
       y = "New Vaccinations")

cowplot::plot_grid(tlRus3, tlChi3, tlUSA3, ncol = 1, nrow = 3)
```

This time lapse graph represents the new number of new vaccinations across all three countries. What is interesting to note is the fact that US had a decreasing number of vaccinations more rapidly than the other two countries.

Times Series for the Cluster Cases

```{r DH5}
#Dataframe for US Cases
UScases = data.frame(df_USA$new_cases)
UScases = UScases[-(1:9),]

#Data frame for Russia Cases
RusCases = data.frame(df_Russia$new_cases)

#Creating the Start Date and Length
start = as.Date("2020-01-31")
end = as.Date("2022-03-05")
dates = seq(from=start,length=765,by="day")

#Create th Time Series Object
US_case_xts = xts(UScases,order.by = dates)
Rus_case_xts = xts(RusCases,order.by = dates)

#View Time Series
plot(US_case_xts, main="US Cases By Day", xlab = "Date", ylab="Cases")
plot(Rus_case_xts,main="Russia Cases By Day",xlab="Date",ylab="Cases")

#Difference Analysis
diff_ts <- US_case_xts - Rus_case_xts
diff_ts[27:34]

# Plot the difference
plot(diff_ts, type='l', col='green', main='Difference between US and Russia', ylab='Difference', xlab='Date')

#Augmented Dickey-Fuller Test
adf.test(US_case_xts)
adf.test(Rus_case_xts)

```

For the last evaluatation, Taking the US to Russia, we developed the difference between the two countries. Ultimately, the US had a much grander issue than the

## Supervised Learning: Data Pre-Processing

### Function for 5-part confusion matrices

In the analyses that follows, several steps will be repeated multiple times, particularly the creation of confusion matrices to assess the accuracy of each model that uses supervised learning. Those steps are defined here as a function, so that later the code can be simplified to one line.

```{r CMHMFunction}

#Function to create a heatmap for the confusion matrix 
cmheatmap <- function(cmtitle, cm) {
  ggplot(cm$table, aes(Reference, Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_viridis(option = "D", direction = -1) +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels= c("Lowest", "Low", "Mid", "High", "Highest")) +
        scale_y_discrete(labels= c("Lowest", "Low", "Mid", "High", "Highest")) + 
        labs(title = paste(cmtitle, ": Confusion Matrix"), 
             subtitle = paste("Accuracy:", round(cm$overall[1], digits = 2)))
}

```

### Handle NAs

```{r HandleNA}
#remove aggregate rows (rows that are not representing a country, but a region)
#these all have continent = NA
df <- df %>%
  filter(!is.na(continent))

#keep only data presented as rates (remove raw counts)

df <- df %>%
  select(c(iso_code, continent, location, date, aged_65_older, aged_70_older, cardiovasc_death_rate, diabetes_prevalence, extreme_poverty, female_smokers, gdp_per_capita, handwashing_facilities, human_development_index, life_expectancy, male_smokers, median_age, icu_patients_per_million, hosp_patients_per_million, hospital_beds_per_thousand, population, population_density, stringency_index, reproduction_rate, new_tests_per_thousand, total_tests_per_thousand, positive_rate, tests_per_case, new_cases_per_million, total_cases_per_million, weekly_hosp_admissions_per_million, weekly_icu_admissions_per_million, new_deaths_per_million, total_deaths_per_million, excess_mortality, new_people_vaccinated_smoothed_per_hundred, new_vaccinations_smoothed_per_million, people_fully_vaccinated_per_hundred, people_vaccinated_per_hundred, total_vaccinations_per_hundred, total_boosters_per_hundred))

#identify how much data is missing in each row/ observation
df$blanks <- rowSums(is.na(df))
dim(df)
hist(df$blanks, main = "Hist of NA values per observation out of 41 attributes")

#identify how much data is missing for each column/ attribute
blankspercolumn <- as.data.frame(row.names = colnames(df), x= colSums(is.na(df)))
hist(blankspercolumn$`colSums(is.na(df))`, main = "Hist of NA values per attribute, out of 156,370 observations")

#remove rows and columns with a high rate of NAs
df <- df %>%
  filter(blanks < 20) %>%
  select(!blanks) %>%
  select(-c("weekly_icu_admissions_per_million", "excess_mortality", "weekly_hosp_admissions_per_million", "total_boosters_per_hundred"))

#remove the blank counting variable, now that it has been used
rm(blankspercolumn)

#Percent missing values in original table
round(sum(is.na(covid))/(dim(covid)[1] * dim(covid)[2]), 2)

#percent missing values in working table, should be lower
round(sum(is.na(df))/(dim(df)[1] * dim(df)[2]), 2)

#create two copies- one to be further processed for mortality measures
dfm <- df 

#create 2nd copy- to be further processed for case measures 
dfc <- df 
```

### Discretize Mortality

Remove variables that are measuring death in different ways, and then check if there are any other unexpected correlations that need to be addressed. Then discretize.

```{r DiscMortality}
#remove observations that have NAs for the outcome, new_deaths_per_million
dfm <- dfm %>%
  filter(!is.na(dfm$new_deaths_per_million))

#remove columns that are also directly counting mortality, they will be highly correlated
dfm <- dfm %>%
  select(-total_deaths_per_million)

#create a correlation matrix identify if any other columns are highly correlated with new deaths per million  
cortable <- as.data.frame(cor(dfm[,5:35], use="pairwise.complete.obs"))
cortable <- data.frame(rownames(cortable), cortable$new_deaths_per_million)

#view the most highly correlated values (both positive and negative correlations)
cortable %>% arrange(desc(cortable.new_deaths_per_million)) %>% head()
cortable %>% arrange(cortable.new_deaths_per_million) %>% head()

#view the distribution of new deaths per million in this dataset
hist(dfm$new_deaths_per_million, main = "Histogram of Mortality", xlab = "Deaths per day, per million")
boxplot(dfm$new_deaths_per_million)
summary(dfm$new_deaths_per_million)

#discretize into 5 categories, roughly equal sized (except the lowest one is large)
deathcategory <- arules::discretize(dfm$new_deaths_per_million, 
                                    method = "frequency", 
                                    breaks = 7, 
                                    ordered_result = TRUE, 
                                    labels = c("Lowest (0-0.1)", "Low (0.1-0.5)", "Mid (0.5-1.3)", "High (1.3-3.8)", "Highest (>3.8)"))
histogram(deathcategory)

#add the discretization to the main table
dfm$deathcategory <- deathcategory

#remove new deaths per million so that's not used in the analysis (we only want the discretized factor)
dfm <- dfm %>% select(-new_deaths_per_million)

#Split into test and train 
set.seed(92)
dfm_training <- sample_frac(dfm, .7)
suppressMessages(dfm_testing <- anti_join(dfm, dfm_training))

#create new tables that don't have the country, month, or actual death number
dfm_training <- dfm_training[,5:35]
dfm_testing <- dfm_testing[,5:35]

```

### Discretize Cases

Create a similar table but where new cases per million is the discretized outcome.

```{r DiscCases}
#remove observations that do not have new_cases_per_million 
dfc <- dfc %>%
  filter(!is.na(dfc$new_cases_per_million)) 

#remove columns that are also counting cases in different ways (or are downstream from case counts, such as death counts)
dfc <- dfc %>%
  select(-c(total_cases_per_million, new_deaths_per_million, total_deaths_per_million, positive_rate, tests_per_case))

#create a correlation matrix identify if any other columns are highly correlated with new cases per million  
cortablec <- as.data.frame(cor(dfc[,5:31], use="pairwise.complete.obs"))
cortablec <- data.frame(rownames(cortablec), cortablec$new_cases_per_million)

#view the most highly correlated values (both positive and negative correlations)
cortablec %>% arrange(desc(cortablec.new_cases_per_million)) %>% head()
cortablec %>% arrange(cortablec.new_cases_per_million) %>% head()

#view the distribution of new cases per million in this dataset
hist(dfc$new_cases_per_million, main = "Histogram of Cases", xlab = "Cases per day, per million")
boxplot(dfc$new_cases_per_million)
summary(dfc$new_cases_per_million)

#discretize
casecategory <- arules::discretize(dfc$new_cases_per_million, 
                                   method = "frequency", 
                                   breaks = 5, 
                                   ordered_result = TRUE, 
                                   labels = c("Lowest (0-1)", "Low (1-11)", "Mid (11-55)", "High (55-198)", "Highest (>198)"))

#view the distribution of cases
histogram(casecategory)

#add the discretization to the main table
dfc$casecategory <- casecategory

#remove new cases per million so that's not used in the analysis 
dfc <- dfc %>% select(-new_cases_per_million)

#Split Case data set into test & train 
set.seed(92)
dfc_training <- sample_frac(dfc, .7)
suppressMessages(dfc_testing <- anti_join(dfc, dfc_training))

#create new tables that don't have the country, month, or actual case number
dfc_training <- dfc_training[,5:31]
dfc_testing <- dfc_testing[,5:31]
```

### Create "Complete Cases" table

```{r CompleteCases}
#remove rows and columns with a high rate of NAs
dfc_cc_testing <- dfc_testing %>%
  select(-c("extreme_poverty", "handwashing_facilities", "icu_patients_per_million", "hosp_patients_per_million"))

dfc_cc_training <- dfc_training %>%
  select(-c("extreme_poverty", "handwashing_facilities", "icu_patients_per_million", "hosp_patients_per_million"))


dfc_cc_testing <- dfc_cc_testing[complete.cases(dfc_cc_testing), ]

dfc_cc_training <- dfc_cc_training[complete.cases(dfc_cc_training), ]


#remove rows and columns with a high rate of NAs
dfm_cc_testing  <- dfm_testing %>%
  select(-c("extreme_poverty", "handwashing_facilities", "icu_patients_per_million", "hosp_patients_per_million"))

dfm_cc_training  <- dfm_training %>%
  select(-c("extreme_poverty", "handwashing_facilities", "icu_patients_per_million", "hosp_patients_per_million"))


dfm_cc_testing  <- dfm_cc_testing [complete.cases(dfm_cc_testing ), ]

dfm_cc_training  <- dfm_cc_training [complete.cases(dfm_cc_training ), ]
```

## KNN

### Cases

K nearest neighbor is a supervised machine learning model that works well with classification problems. This model is considered "instance based learning" or "lazy learning" because it does not actually develop a model. Instead, when given a testing data set, it scans the training dataset to find the "nearest neighbors" (the observations that are most similar to to testing data), and then takes the classification that is most common amongst those neighbors. The main parameter to tune here is the value for K, which is the number of observations that are counted as neighbors. The plot below shows that the best number of neighbors to use for this particular dataset is 7.

```{r KNNCases, cache=TRUE}

#Create the KNN model
knnc_model <- caret::train(
  casecategory~.,
  data = dfc_cc_training,
  method = "knn",
  na.action = na.omit
)

#plot the tuned model to see accuracy against number of neighbors tested
plot(knnc_model) #best number of neighbors was 7

#create predictions on the training data
knnc_training_predictions <- predict(knnc_model, dfc_cc_training)

#use a confusion matrix to assess accuracy on training data
knnc_training_cnfsmtrx <- confusionMatrix(dfc_cc_training$casecategory, knnc_training_predictions)
knnc_training_cnfsmtrx$overall[1]
```

The Confusion Matrix below shows that this model is able to predict the daily case rate with a 73% accuracy level.

```{r}
#create predictions on the testing data
knnc_testing_predictions <- predict(knnc_model, dfc_cc_testing)

#use a confusion matrix to assess
knnc_testing_cnfsmtrx <- confusionMatrix(dfc_cc_testing$casecategory, knnc_testing_predictions)

#visualize the confusion matrix on the testing data
cmheatmap("KNN (Cases)", knnc_testing_cnfsmtrx)
```

### Mortality

When reproducing the KNN model to predict mortality rather than cases, the best number of neighbors to use in the analysis is 9 instead of 7.

```{r KNNMortality, cache=TRUE}
#Create the knn model
knnm_model <- caret::train(
  deathcategory~.,
  data = dfm_cc_training ,
  method = "knn",
  na.action = na.omit
)

#plot the tuned model to see accuracy against number of neighbors tested
plot(knnm_model) #best number of neighbors was 9

#create predictions on the training data
knnm_training_predictions <- predict(knnm_model, dfm_cc_training )

#use a confusion matrix to assess accuracy on training data
knnm_training_cnfsmtrx <- confusionMatrix(dfm_cc_training $deathcategory, knnm_training_predictions)
knnm_training_cnfsmtrx$overall[1]

```

The accuracy on the training data for mortality is 74%. The Confusion Matrix below shows that the accuracy on the testing data is 68%.

```{r}

#create predictions on the testing data
knnm_testing_predictions <- predict(knnm_model, dfm_cc_testing )

#use a confusion matrix to assess
knnm_testing_cnfsmtrx <- confusionMatrix(dfm_cc_testing $deathcategory, knnm_testing_predictions)

#visualize the confusion matrix on the testing data
cmheatmap("KNN (Mortality)", knnm_testing_cnfsmtrx)
```

### KNN Mortality on one country only

As a next step, this study will check to see if the accuracy of the model might improve when it is only assessing the mortality rate for one particular country at a time. In this instance, Italy will be used. It can be seen below that the best number of neighbors to use is again 9.

```{r KNNMortality1country, warning=FALSE, cache=TRUE}

#Population of Italy is 60367471
dfm_1c_training <- filter(dfm_cc_training , population == 60367471)

dfm_1c_testing <- filter(dfm_cc_testing ,  population == 60367471)

#Create the knn model for Italy only
knnm_IT_model_tuned <- caret::train(
  deathcategory~.,
  data = dfm_1c_training, 
  method = "knn", 
  na.action = na.omit
)

#plot the tuned model to see accuracy against number of neighbors tested
plot(knnm_IT_model_tuned) #best number of neighbors was 9

#create predictions on the training data
knnm_IT_training_predictions <- predict(knnm_IT_model_tuned, dfm_1c_training)

#use a confusion matrix to assess accuracy on training data
knnm_IT_training_cnfsmtrx <- confusionMatrix(dfm_1c_training$deathcategory, knnm_IT_training_predictions)
knnm_IT_training_cnfsmtrx$overall[1]

#create predictions on the testing data
knnm_IT_testing_predictions <- predict(knnm_IT_model_tuned, dfm_1c_testing)

#use a confusion matrix to assess
knnm_IT_testing_cnfsmtrx <- confusionMatrix(dfm_1c_testing$deathcategory, knnm_IT_testing_predictions)

#visualize the confusion matrix on the testing data
cmheatmap("KNN (Italy Mortality)", knnm_IT_testing_cnfsmtrx)
```

As can be seen above, when The Confusion Matrix below shows that this model is able to predict the daily case rate with a 73% accuracy level. This is an interesting finding which will be tested on other models further along in this paper as well.

## Naive Bayes

Naive Bayes is a supervised machine learning algorithm that uses statistics to solve classification problems. This model works by calculating the probability that an unknown data point belongs to a certain class (outcome), assuming that all features in the model are independent of each other (have no combined effects upon the outcome). In this case, the outcomes of interest will be both the daily mortality rate and case count discretized into 5 levels from lowest to highest. The predictors used to train the model will be all variables included in the data set such as the number of new vaccinations, number of tests, hospital admissions, the stringency index, the reproduction number of the virus at that time, and static country characteristics such as the Gross Domestic Product (GDP), population and life expectancy. During pre-processing, the data was split into training and testing data. After the model is trained on the training data, the testing data will be used to assess how accurate the model is at predicting what level of mortality and/or cases a certain country would experience on any given day, based on the other factors.

The algorithm will calculate the mean and standard deviation for each predictor, and compare that against the "apriori," which is the likelihood of an observation randomly falling into each category. The first outcome to be assessed in this study will be mortality.

### Mortality

```{r NBMortality, cache=TRUE}
#create the basic naive bayes model
nbm_model <- e1071::naiveBayes(deathcategory~.,
                       data = dfm_training)

#create a prediction based on the training data 
nbm_training_predictions <- predict(nbm_model, dfm_training)

#create a prediction on the testing data 
nbm_testing_predictions <- predict(nbm_model, dfm_testing)

#review the apriori, which shows the number of observations within each class
nbm_model$apriori


```

The apriori figure above shows the distribution of training data observations in each category. It should be noted that the lowest bin, with a daily mortality rate below 0.1 has a larger share of observations in it, due to the skew of the original data.

```{r}
#check how well the prediction did on training data using a confusion matrix 
nbm_training_cnfsmtrx <- confusionMatrix(dfm_training$deathcategory, nbm_training_predictions)
nbm_training_cnfsmtrx$overall[1]
```

The model only reached a 41% accuracy level when using the training data. Below, the model will be tested against the testing data set, which was not used to train the model.

```{r}
#check how well the prediction did on testing data using a confusion matrix 
nbm_testing_cnfsmtrx <- confusionMatrix(dfm_testing$deathcategory, nbm_testing_predictions)
nbm_testing_cnfsmtrx$overall
```

The output of the Confusion Matrix shows that the "No Information Rate" for this model would be 27%, meaning that if each observation was randomly assigned to a group with no information available to help make predictions, it would likely still be correct about 27% of the time. The model's accuracy is 41%, so it does perform better than it would were there be to no information available. The Confusion Matrix is represented visually below, and it can be seen that the model appears to be predicting that mortality would be lower than it actually is.

```{r}
#visualize the confusion matrix on the testing data
cmheatmap("Naive Bayes (Mortality)", nbm_testing_cnfsmtrx)
```

The following tables show the mean and standard deviation of a few key variables for each classification. It is interesting to note that as the GDP of a country increases, it appears that the daily mortality rate is also increasing, so in this dataset, wealthier countries experienced (or at least reported) more daily mortality than lower income countries.

Not surprisingly, the data below shows that the mortality rate increases as the cases increase. The mean number of new cases per million on a day with the lowest mortality was only 57, while the mean number of cases on a day with the highest mortality levels was 566.

```{r}
#look at some of the probabilities (first column is mean for that class, second is standard deviation)
nbm_model$tables$gdp_per_capita

nbm_model$tables$new_cases_per_million
```

### Naive Bayes on cases

Because the daily case rate appears to be a strong predictor of the daily mortality rate, a model will be developed to attempt to predict the daily case rate as well. The process will be identical as above, using NaÃ¯ve Bayes again.

```{r NBCases, cache=TRUE}
#create the basic naive bayes model
nbc_model <- e1071::naiveBayes(casecategory~.,
                       data = dfc_training)

#create a prediction based on the training data 
nbc_training_predictions <- predict(nbc_model, dfc_training)

#create a prediction on the testing data 
nbc_testing_predictions <- predict(nbc_model, dfc_testing)

#review the apriori, which shows the number of observations within each class
nbc_model$apriori

#check how well the prediction did using a confusion matrix 
nbc_training_cnfsmtrx <- confusionMatrix(dfc_training$casecategory, nbc_training_predictions)
nbc_training_cnfsmtrx$overall[1]


```

The NaÃ¯ve Bayes model is only able to predict the daily case rate on training data at an accuracy level of 39%. This is slightly worse than the model to predict daily mortality. The model will be assessed against the testing data as well.

```{r}
#check how well the prediction did using a confusion matrix 
nbc_testing_cnfsmtrx <- confusionMatrix(dfc_testing$casecategory, nbc_testing_predictions)
nbc_testing_cnfsmtrx$overall

#visualize the confusion matrix on the testing data
cmheatmap("Naive Bayes (Cases)", nbc_testing_cnfsmtrx)
```

The accuracy level for the NaÃ¯ve Bayes model on testing data is still only 39%. The No Information Rate is 31%, so this model does not perform much better than randomly assigning observations to an outcome group. The heatmap below provides a visual representation of the Confusion Matrix. It should be noted that this particular model is markedly misclassifying days where the case rate is in the mid-range.

The table below show a similar trend where the mean GDP appears to also increase as the daily case rate increases, indicating that countries with a higher incoming are experience (or at least reporting) more deaths.

```{r}
#look at some of the probabilities (first column is mean for that class, second is standard deviation)
nbc_model$tables$gdp_per_capita
```

### Tuned Naive Bayes (Cases)

Both NaÃ¯ve Bayes models (predicting mortality and case rates) performed poorly, only slightly outperforming what could be expected randomly. NaÃ¯ve Bayes models have parameters that can be tuned though, so the code below will use a tuning grid to try several different iterations of the model, to find the best combination of parameters. In order to use the tuning grid, the data required some additional pre-processing, including removing observations that did not have complete data available. This modified dataset will be used to train this model.

```{r NBTunedCases, cache=TRUE}
#create a tuning grid
nb_grid <- expand.grid(usekernel = c(TRUE, FALSE),
                       laplace = c(0, 0.5, 1), 
                       adjust = c(0.75, 1, 1.25, 1.5))

# Create naive bayes tuned model
set.seed(92)
nbc_model_tuned <- train(casecategory~.,
                                data = dfc_cc_training,
                                method = "naive_bayes",
                                usepoisson = TRUE,
                                tuneGrid = nb_grid,
                                na.action = "na.omit")

# View the selected tuning parameters
nbc_model_tuned$finalModel$tuneValue
```

The tuning grid identified the best combination of parameters to increase the accuracy of the NaÃ¯ve Bayes model on this data set. The best combination of parameters is to have laplace smoothing set at 0, to use a kernel, and set the adjustment at 0.75. The model created with these parameters is assessed below.

```{r}
#create a prediction based on the training data 
nbc_tuned_training_predictions <- predict(nbc_model_tuned, dfc_cc_training)

#create a prediction on the testing data 
nbc_tuned_testing_predictions <- predict(nbc_model_tuned, dfc_cc_testing)

#check how well the prediction did using a confusion matrix 
nbc_tuned_training_cnfsmtrx <- confusionMatrix(dfc_cc_training$casecategory, nbc_tuned_training_predictions)

#check how well the prediction did using a confusion matrix 
nbc_tuned_testing_cnfsmtrx <- confusionMatrix(dfc_cc_testing$casecategory, nbc_tuned_testing_predictions)
nbc_tuned_testing_cnfsmtrx

#visualize the confusion matrix on the testing data
cmheatmap("Tuned Naive Bayes (Cases)", nbc_tuned_testing_cnfsmtrx)
```

The tuned NaÃ¯ve Bayes model had a 52% accuracy, which is an improvement over the un-tuned model's accuracy of 39%. Part of this improvement could also be due to the dataset used to train the model, which includes only complete cases, all observations with missing data were excluded from this analysis.

## Decision Tree

### Mortality

A Decision Tree is another machine learning algorithm which can be used for classification tasks. It takes labeled data, and then progressively splits the data based on certain decision points. This creates a visual tree-like structure with internal nodes (decision points), branches (rules), and leaf nodes (final result/ classification). A Decision Tree is a popular machine learning algorithm because it is very easily interpreted by the human mind, as it shows a step by step process for determining how to classify the outcome. Below a Decision Tree model will be developed to predict both the daily case rate, and the daily mortality rate.

```{r DTMortality, cache=TRUE}
#create the decision tree
dtm_model <- rpart::rpart(deathcategory~.,
                       data = dfm_training,
                      method = "class")

#create a prediction based on the training data 
dtm_training_predictions <- predict(dtm_model, dfm_training, type = "class")

#create a prediction on the testing data 
dtm_testing_predictions <- predict(dtm_model, dfm_testing, type = "class")

#view the most important variables
as.data.frame(head(dtm_model$variable.importance))


```

One important thing to review with Decision Trees is the Variable Importance. This identifies which variables in the model were most predictive. It can be seen above that the most important variables for predicting the daily mortality rate are the number of new cases and total cases, followed by four variables that are specific to each country (rather than to a particular day) such as the human development index, life expectancy, GDP and median age.

The actual Decision Tree can be seen below. The first node asks if the new cases per million were above or below 5.5. If they were below that, then the mortality rate for that day was most likely in the lowest category. If they are above that figure, then the next node asks if they were above or below 105. After that, there are several more splits looking at population, hospitalizations, new cases again, and GDP.

```{r}
#view the decision tree (using the base plot() function)
plot(dtm_model)
text(dtm_model, cex = 0.7)

#view the decision tree (trying another different versions)
rattle::fancyRpartPlot(dtm_model, cex = 0.5, main = "Decision Tree (Mortality)") 

#check how well the prediction did on the training data using a confusion matrix 
dtm_training_cnfsmtrx <- confusionMatrix(dfm_training$deathcategory, dtm_training_predictions)
dtm_training_cnfsmtrx$overall[1]

#check how well the prediction did on testing using a confusion matrix 
dtm_testing_cnfsmtrx <- confusionMatrix(dfm_testing$deathcategory, dtm_testing_predictions)

#visualize the confusion matrix on the testing data
cmheatmap("Decision Tree: (Mortality)", dtm_testing_cnfsmtrx)
```

The Decision Tree model has an accuracy rate of 62% on predicting mortality using the testing data. Because one of the main predictors for mortality was cases, the predictors for daily case rate will also be assessed using a decision tree model as well.

### Decision tree on case category

```{r DTCases, cache=TRUE}
#create the decision tree
dtc_model <- rpart::rpart(casecategory~.,
                       data = dfc_training,
                      method = "class")

#create a prediction based on the training data 
dtc_training_predictions <- predict(dtc_model, dfc_training, type = "class")

#create a prediction on the testing data 
dtc_testing_predictions <- predict(dtc_model, dfc_testing, type = "class")

#view the most important variables
as.data.frame(head(dtc_model$variable.importance))
```

The Variable Importance measure shows that the factors that impact the prediction most include country-specific features such as the human development index, life expectancy, GDP, and the population's age. Factors that change by day, such as the number of new tests are also included. The actual tree will be visualized below.

The first split in the decision tree is looking at the human development index (HDI). For countries with a lower HDI, the next factor to consider is the reproduction rate of the virus. For countries with a higher HDI, the next factor to consider is the number of new tests per thousand citizens.

```{r}
#view the decision tree (using the base plot() function)
plot(dtc_model)
text(dtc_model, cex = 0.7)

#view the decision tree (trying another different versions)
rattle::fancyRpartPlot(dtc_model, cex = 0.5, main = "Decision Tree (Cases)") 

#check how well the prediction did on the training data using a confusion matrix 
dtc_training_cnfsmtrx <- confusionMatrix(dfc_training$casecategory, dtc_training_predictions)
dtc_training_cnfsmtrx$overall[1]

#check how well the prediction did using a confusion matrix 
dtc_testing_cnfsmtrx <- confusionMatrix(dfc_testing$casecategory, dtc_testing_predictions)

#visualize the confusion matrix on the testing data
cmheatmap("Decision Tree (Cases)", dtc_testing_cnfsmtrx)
```

The accuracy rate for the Decision Tree on daily cases is only 43%, and an interesting pattern appears. This model is not capturing days where the daily cases are in the mid-range. Because it was noted that many of the factors considered in this model are related to the country, rather than to the individual day, the next model will attempt to recreate a decision tree but separating countries out. The United States will be singled out in this case, to see what factors the model uses to predict the daily case rate when all of the country specific variables are constant.

### Decision tree on one country

```{r DTCases1country, cache=TRUE}

#United States population is 332915074
dfc_1c_training <- filter(dfc_training, population == 332915074)
dfc_1c_testing <- filter(dfc_testing,  population == 332915074)

#create the decision tree
dtc_1c_model <- rpart::rpart(casecategory~.,
                       data = dfc_1c_training,
                      method = "class")

#create a prediction based on the training data 
dtc_1c_training_predictions <- predict(dtc_1c_model, dfc_1c_training, type = "class")

#create a prediction on the testing data 
dtc_1c_testing_predictions <- predict(dtc_1c_model, dfc_1c_testing, type = "class")

#view the most important variables
as.data.frame(head(dtc_1c_model$variable.importance))
```

The variables that are most important when looking at only one country (the United States) are the number of tests, hospitalizations, the stringency index and reproduction rate. The tree can be viewed below. The first decision is the number of new tests, followed by the stringency index.

```{r}
#view the decision tree (using the base plot() function)
plot(dtc_1c_model)
text(dtc_1c_model, cex = 0.7)

#view the decision tree (trying another different versions)
rattle::fancyRpartPlot(dtc_1c_model, cex = 0.5, main = "Decision Tree (Cases)") 

#check how well the prediction did on the training data using a confusion matrix 
dtc_1c_training_cnfsmtrx <- confusionMatrix(dfc_1c_training$casecategory, dtc_1c_training_predictions)
dtc_1c_training_cnfsmtrx$overall[1]

#check how well the prediction did using a confusion matrix 
dtc_1c_testing_cnfsmtrx <- confusionMatrix(dfc_1c_testing$casecategory, dtc_1c_testing_predictions)

#visualize the confusion matrix on the testing data
cmheatmap("Decision Tree (US Cases)", dtc_1c_testing_cnfsmtrx)
```

When limiting the Decision Tree model to one country at a time, the accuracy rate greatly increases. This model is able to predict the daily case rate with an 88% accuracy for the United States.

### Decision Tree Cases, One Country, Pruned

Now that a strong model has been identified, the model can be further tuned. The first step in pruning a decision tree is to assess the complexity factor, and the best level. The visual below indicates that the best level of complexity to cut the tree at would be 0.019.

```{r DTC1cPruned, cache=TRUE}
#plot the complexity parameter
rpart::plotcp(dtc_1c_model)

#prune the model based on the complexity parameter
dtc_1c_model_pruned <- rpart::prune(dtc_1c_model, cp = 0.019)

#create a prediction based on the training data 
dtc_1cp_training_predictions <- predict(dtc_1c_model_pruned, dfc_1c_training, type = "class")

#check how well the prediction did using a confusion matrix 
dtc_1cp_training_cnfsmtrx <- confusionMatrix(dfc_1c_training$casecategory, dtc_1cp_training_predictions)
dtc_1cp_training_cnfsmtrx$overall[1]

#create a prediction on the testing data 
dtc_1cp_testing_predictions <- predict(dtc_1c_model_pruned, dfc_1c_testing, type = "class")

#check how well the prediction did on testing data using a confusion matrix 
dtc_1cp_testing_cnfsmtrx <- confusionMatrix(dfc_1c_testing$casecategory, dtc_1cp_testing_predictions)

#visualize the confusion matrix on the testing data
cmheatmap("Decision Tree Pruned", dtc_1cp_testing_cnfsmtrx)
```

The Confusion Matrix above shows that the accuracy level is still at 88%. This may be because the initial model developed without tuning was actually already performing as well as this type of model could with the data available.

# Results

The COVID-19 pandemic is something that impacted the lives of nearly every person on the planet. This type of global pandemic does not occur often, so it is important for public health professionals to be able to review the available data and use it to prepare for the next public health crisis.

Machine Learning techniques can help data scientists to find hidden trends in the data that might otherwise be difficult to identify. In this study, machine learning techniques were used to try and predict the burden of disease on a given population over time. The daily case rate and daily mortality rate were both used as outcome measures, and a variety of variables related to each individual nation's overall status, and time sensitive daily changes were used as predictors. First, an exploratory data analysis was conducted to identify any potential issues with the data, or preliminary trends. A k-means clustering algorithm was used to cluster the countries by their lasting characteristics (life expectancy, hospital bed availability, human development index, extreme poverty etc), and key countries from each cluster were compared to look for differences overtime.

The data was pre-processed to remove excessive missing data before splitting it into training and testing data sets. Once split, it was used to train and then test NaÃ¯ve Bayes, Decision Tree and K Nearest Neighbor models. A model was created to predict cases and another to predict mortality with each algorithm. In addition, each algorithm was then tuned to try and improve the accuracy of the model. A summary of the models developed can be found below.

-   K Nearest Neighbor on Cases, using the caret package's knn model, with k=7.

-   K Nearest Neighbor on Mortality, using the caret package's knn model, with k=9.

-   KNN on Mortality, for one country only (Italy), using the caret package's knn model, with k=9

-   Naive Bayes on Mortality, using the standard parameters of the e1071 package.

-   Naive Bayes on Cases, using the standard parameters of the e1071 package.

-   Naive Bayes on Cases, tuned using the caret package.

-   Decision Tree on Mortality, using the rpart package and standard parameters.

-   Decision Tree on Cases, using the rpart package and standard parameters.

-   Decision tree on the US Cases only

-   Decision Tree on US cases only, pruned using the Complexity Parameter.

The table below shows the accuracy results of the confusion matrices for each model, including the default version and all tuned models that were developed as well.

```{r ResultsTable}

#Gather accuracy results for all of the final testing models for each algorithm
rtTestingResults <- c(nbc_testing_cnfsmtrx$overall[1],
      nbm_testing_cnfsmtrx$overall[1],
      nbc_tuned_testing_cnfsmtrx$overall[1],
      dtc_testing_cnfsmtrx$overall[1],
      dtm_testing_cnfsmtrx$overall[1],
      dtc_1c_testing_cnfsmtrx$overall[1],
      dtc_1cp_testing_cnfsmtrx$overall[1],
      knnm_testing_cnfsmtrx$overall[1],
      knnc_testing_cnfsmtrx$overall[1], 
      knnm_IT_testing_cnfsmtrx$overall[1]
      )

#Gather accuracy results for all of the final training models for each algorithm
rtTrainingResults <- c(nbc_training_cnfsmtrx$overall[1],
      nbm_training_cnfsmtrx$overall[1],
      nbc_tuned_training_cnfsmtrx$overall[1],
      dtc_training_cnfsmtrx$overall[1],
      dtm_training_cnfsmtrx$overall[1],
      dtc_1c_training_cnfsmtrx$overall[1],
      dtc_1cp_training_cnfsmtrx$overall[1],
      knnm_training_cnfsmtrx$overall[1],
      knnc_training_cnfsmtrx$overall[1], 
      knnm_IT_training_cnfsmtrx$overall[1]
      )

#Create a vector with each model's name
rtCategories <- c("Naive Bayes Cases", "Naive Bayes Mortality", "Naive Bayes Tuned Cases" ,"Decision Tree Cases", "Decision Tree Mortality", "Decision Tree: 1 country", "Decision Tree Cases, 1 country, Tuned","KNN Mortality", "KNN Cases", "KNN Italian Mortality")

#create a table showing training and testing results for the models
rtResultTable <- data.frame(rtCategories, rtTrainingResults, rtTestingResults)

names(rtResultTable) <- c("Model Results", "Training Accuracy", "Testing Accuracy")

rtResultTable <- mutate_if(rtResultTable, is.numeric, round, digits =2)

rtResultTable
```

While the training accuracy is interesting, the value that is most impactful is the testing accuracy. This is how well each model performed on data it had not yet seen. The best performing model was the Decision Tree model, when limited to one specific country. Whether the model was tuned or not, it achieved the same level of accuracy at 88%. KNN also performed well when looking at one country, with 85% accuracy. The least accurate model was the un-tuned Naive Bayes model predicting cases, which could only correctly classify 39% of the testing data.

# Conclusions

After extensive research of determining different factors about COVID-19, there was some interesting and peculiar information that was landed upon. The biological horror of COVID-19 was one of a kind, potentially any other pandemics we have ever seen. In today's society, there is an estimate that 7 million people have passed away due to the virus ever since the pandemic. COVID-19 had an incredible infection rate while its predecessor, the Spanish Flu, was more deadly. The reason why this is important to note is that, history tends to repeat itself. In the future, we cannot surely say that there will never be another pandemic because viruses have been around since the beginning of time. These resilient and adaptable beings have always found ways to become more powerful towards many living species. It is their way of life to infect and evolve. 

The idea of this project was to determine key factors regarding how COVID-19 affected the different countries of the world. As stated before, the previous pandemic was the Spanish flu that took the lives of more than 20 million people! In terms of the two viruses, the procedures and processes of safety have improved. However, it is pertinent to continue improving to preserve more life down the way. In this study, the mortality (death rate), and incidence (case rate) of the disease will be the primary outcomes assessed. The available data will be evaluated to look for any trends that identify which factors lead to increased mortality and increased incidence of disease. Here are some of the key factors we noticed between our study.

- The range for daily mortality ranges from 0 to 453 deaths per million citizens
- The daily mortality rate increased as the amount of hospitalizations increased.
- The stringency index shows that the mortality rate decreases if a country has a higher stringency index
- There were three main clusters that included GDP, Life Expectancy, and Stringency Index 

Each of the factors stated above determined how dangerous COVID-19 was to their economy. For example, China was able to contain the virus in the earlier stages to prevent any kind of further detriment. At the same time, Russia and the USA had much more difficult times containing the virus throughout the course of the pandemic. We also must take into fact the government itself. Though the stringency index can be quantified to a certain level, it cannot fully ascertain the volatility of a government. Multiple decisions are made based on the experience and each government is different from their counterparts. However, what we can deem is that the effective and swift utilization of protocol such as minimizing transportation, reducing foreign involvements, etc can lead to a more contained environment.

After looking for relationships and correlations, our next step was to take our analysis further with machine learning. Machine learning is a powerful tool in data analytics. Through various tools, we have the capability of categorizing, predicting, etc. Our goal for the project was to see if we could build a model that can determine outcomes using different aspects for the data. Here are the results we found from our machine learning methods!

**KNN**
- The Confusion Matrix below shows that the accuracy on the testing data is 68%.
- A model done on a single country (Italy) is able to predict the daily case rate with a 73% accuracy level.

**Naive Bayes**
- The model's accuracy is 41%, so it does perform better than it would were there be to no information available.
- The tuned NaÃ¯ve Bayes model had a 52% accuracy, which is an improvement over the un-tuned model's accuracy of 39%.

**Decision Tree**
- The Decision Tree model has an accuracy rate of 62% on predicting mortality using the testing data. Because one of the main predictors for mortality was cases, the predictors for daily case rate will also be assessed using a decision tree model as well.
- The accuracy rate for the Decision Tree on daily cases is only 43%, and an interesting pattern appears. This model is not capturing days where the daily cases are in the mid-range.
- When limiting the Decision Tree model to one country at a time, the accuracy rate greatly increases. This model is able to predict the daily case rate with an 88% accuracy for the United States.
- Even when pruned, The Confusion Matrix above shows that the accuracy level is still at 88%.


In conclusion, global pandemics are always unpredictable and volatile. There are a number of circumstances that play into various factors. Even though understanding these factors, it can still be difficult to completely comprehend and hone in. However, data analytics does allow us to mitigate them. By performing all the research from COVID-19, we can develop appropriate measures and ideas from the results alone. For example, we were able to get an accuracy level of 88% for one of our machine learning tasks! Once again, there is the issue of obtaining everything as data and setbacks in terms of overfitting. But, we can definitely combat that by making sure to learn from our previous endeavors. Imagine what could happen if we were able to take data from various pandemics to build upon! Ultimately, this project does provide useful information on a public safety level. It provides the opportunity for insight and improvement as a whole!

```{r Runtime}
end_time <- Sys.time() #adding this just to track how long this takes
paste("This took",round(end_time - start_time, digits = 2), "minutes to run")
```
